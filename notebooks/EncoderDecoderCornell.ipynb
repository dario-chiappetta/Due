{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder/Decoder Dialogue Management\n",
    "\n",
    "Here we use a simple Encoder/Decoder GRU network to predict answers from the Cornell Movie-Dialog Corpus. We use **PyTorch** as a deep learning framework.\n",
    "\n",
    "Most of the code in this notebook comes from the following **tutorial** on English-French translation.\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "We apply the Machine Translation framework to Dialogue Management by encoding single sentences in the corpus, and decoding their answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset \n",
    "\n",
    "We start loading the corpus' dialogs as **Episodes** (class due.episode.Episode). We limit the number of episodes to load so we can test the code more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/83097 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from due.corpora import cornell\n",
    "import itertools\n",
    "\n",
    "N_DIALOGS = 100\n",
    "\n",
    "episodes = list(itertools.islice(cornell.episode_generator(), N_DIALOGS))\n",
    "\n",
    "# episodes = cornell.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Event(type=<Type.Utterance: 'utterance'>, timestamp=datetime.datetime(2011, 6, 15, 12, 4, 49), agent='u9', payload=\"What's the worst?\"),\n",
       " Event(type=<Type.Utterance: 'utterance'>, timestamp=datetime.datetime(2011, 6, 15, 12, 4, 50), agent='u2', payload='You get the girl.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[95].events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning\n",
    "\n",
    "Here we define functions for a simple text processing pipeline, where we just convert sentences to lowercase and tokenize them using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "# Needed: pipenv run python -m spacy download en\n",
    "spacy_nlp_en = spacy.load('en')\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    s_spacy = spacy_nlp_en(sentence)\n",
    "    return [str(token) for token in s_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad . again .\n"
     ]
    }
   ],
   "source": [
    "def normalize_sentence(sentence, return_tokens=False):\n",
    "    result = sentence.lower()\n",
    "    result = re.sub(r'\\s+', ' ', result)\n",
    "    result = tokenize_sentence(result)\n",
    "    if not return_tokens:\n",
    "        result = ' '.join(result)\n",
    "    return result\n",
    "\n",
    "s_normalized = normalize_sentence(s, False)\n",
    "print(s_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "Here we generate a dataset of utterances and their responses. The **output** of this section is:\n",
    "\n",
    "* A list of utterances (`str`) `X`    \n",
    "* A list of responses (`str`) `y`, one per utterance in `X`.\n",
    "\n",
    "Example:\n",
    "\n",
    "* X: `[\"hi\", \"hello how are you?\", \"i'm fine thanks\", ...]`\n",
    "* y: `[\"hello how are you?\", \"i'm fine thanks\", \"good to hear\", ...]`\n",
    "\n",
    "Note that within an Episode `i`, `y_i` is just `X_i[1:]`. This is not true when `X` and `y` are obtained concatenating data from multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from due.event import Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad . again .',\n",
       "  \"well , i thought we 'd start with pronunciation , if that 's okay with you .\",\n",
       "  'not the hacking and gagging and spitting part . please .'],\n",
       " [\"well , i thought we 'd start with pronunciation , if that 's okay with you .\",\n",
       "  'not the hacking and gagging and spitting part . please .',\n",
       "  \"okay ... then how 'bout we try out some french cuisine . saturday ? night ?\"])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _is_utterance(event):\n",
    "    return event.type == Event.Type.Utterance\n",
    "\n",
    "def extract_pairs(episode):\n",
    "    \"\"\"\n",
    "    Process Events in an Episode, extracting all the Event pairs that can be interpreted as one\n",
    "    dialogue turn (ie. an Agent's utterance, and another Agent's response)\n",
    "    \n",
    "    In particular, Event pairs are extracted so that:\n",
    "    \n",
    "    * Both Events are Utterances (currently, non-utterances will raise an exception)\n",
    "    * The second Event immediately follows the first\n",
    "    * The two Events are acted by two different Agents\n",
    "    \n",
    "    Two lists of the same length are returned, so that each utterance (`str`) in the first list\n",
    "    has its response in the second.\n",
    "    \"\"\"\n",
    "    alternate_sentences = [episode.events[0].payload]\n",
    "    for e1, e2 in zip(episode.events, episode.events[1:]):\n",
    "        if not _is_utterance(e1) or not _is_utterance(e2):\n",
    "            raise NotImplementedError(\"Non-utterance Events are not supported yet\")\n",
    "            \n",
    "        if e1.agent != e2.agent:\n",
    "            alternate_sentences.append(e2.payload)\n",
    "\n",
    "    normalized_alternate_sentences = [normalize_sentence(s) for s in alternate_sentences]\n",
    "\n",
    "    result_X = normalized_alternate_sentences[0:-1]\n",
    "    result_y = normalized_alternate_sentences[1:]\n",
    "    \n",
    "    return result_X, result_y\n",
    "    \n",
    "extract_pairs(episodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 25.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for e in tqdm(episodes):\n",
    "    try:\n",
    "        episode_X, episode_y = extract_pairs(e)\n",
    "    except AttributeError:\n",
    "        print(\"Skipping episode with events: %s\" % e.events)\n",
    "    X.extend(episode_X)\n",
    "    y.extend(episode_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "Here we index all the words in the corpus so that we can associate each word with a numeric ID, and vice versa.\n",
    "\n",
    "**TODO**: consider using torchtext instead\n",
    "\n",
    "**TODO**: implement pruning of rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Vocabulary():\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.index_to_count = defaultdict(int)\n",
    "        self.current_index = 1\n",
    "        \n",
    "        self.add_word('<UNK>') # Unknown token\n",
    "        self.add_word('<SOS>') # Start of String\n",
    "        self.add_word('<EOS>') # End of String\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Add a new word to the dictionary.\n",
    "        \"\"\"\n",
    "        if word in self.word_to_index:\n",
    "            index = self.word_to_index[word]\n",
    "        else:\n",
    "            index = self.current_index\n",
    "            self.current_index += 1\n",
    "            self.word_to_index[word] = index\n",
    "            self.index_to_word[index] = word\n",
    "            \n",
    "        self.index_to_count[index] += 1\n",
    "        \n",
    "    def index(self, word):\n",
    "        \"\"\"\n",
    "        Retrieve a word's index in the Vocabulary. Return the index of the <UNK>\n",
    "        token if not present.\n",
    "        \"\"\"\n",
    "        if word in self.word_to_index:\n",
    "            return self.word_to_index[word]\n",
    "        return self.word_to_index['<UNK>']\n",
    "    \n",
    "    def word(self, index):\n",
    "        \"\"\"\n",
    "        Return the word corresponding to the given index/\n",
    "        \"\"\"\n",
    "        return self.index_to_word[index]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1, '<SOS>': 2, '<EOS>': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = Vocabulary()\n",
    "vocabulary.word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in set(X + y):\n",
    "    for word in sentence.split():\n",
    "        vocabulary.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "We could initialize the model's embedding layer with random weights, but we expect better results using pre-trained word embeddings instead. We chose **GloVe** 6B, 300d word vectors for this purpose.\n",
    "\n",
    "To set these vectors as default embeddings for our network we need to prepare a matrix of `(vocabulary_size, embedding_dim)` elements where the *i*-th row is the embedding vector of the word of index *i* in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from due import resource_manager\n",
    "rm = resource_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(vocabulary):\n",
    "    with rm.open_resource_file('embeddings.glove6B', 'glove.6B.300d.txt') as f:\n",
    "        unk_index = vocabulary.index('<UNK>')\n",
    "        result = np.zeros((vocabulary.size()+1, 300))\n",
    "        for line in tqdm(f):\n",
    "            line_split = line.split()\n",
    "            word = line_split[0]\n",
    "            index = vocabulary.index(word)\n",
    "            if index != unk_index:\n",
    "                vector = [float(x) for x in line_split[1:]]\n",
    "                result[index,:] = vector\n",
    "        sos_index = vocabulary.index('<SOS>')\n",
    "        result[sos_index,:] = np.ones(300)\n",
    "        return torch.FloatTensor(result, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:13, 30004.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding_matrix = get_embedding_matrix(vocabulary).cuda()\n",
    "embedding_matrix = get_embedding_matrix(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "        ...,\n",
       "        [ 0.6605, -0.3381,  0.3872,  ...,  0.4240, -0.1595, -0.0201],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = torch.FloatTensor(np.zeros((vocabulary.size()+1, 300)), device=DEVICE)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "\n",
    "Here we define a function to encode a sentence into a Torch tensor of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_tensor(sentence):\n",
    "    sentence_indexes = [vocabulary.index(w) for w in sentence.split()]\n",
    "    sentence_indexes.append(vocabulary.index('<EOS>'))\n",
    "    return torch.tensor(sentence_indexes, dtype=torch.long, device=DEVICE).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 174],\n",
       "        [  28],\n",
       "        [ 368],\n",
       "        [  58],\n",
       "        [ 484],\n",
       "        [  17],\n",
       "        [ 485],\n",
       "        [ 486],\n",
       "        [  27],\n",
       "        [ 487],\n",
       "        [ 488],\n",
       "        [ 162],\n",
       "        [ 489],\n",
       "        [ 255],\n",
       "        [ 490],\n",
       "        [ 491],\n",
       "        [ 492],\n",
       "        [ 493],\n",
       "        [ 355],\n",
       "        [ 196],\n",
       "        [  92],\n",
       "        [ 494],\n",
       "        [  10],\n",
       "        [ 328],\n",
       "        [  10],\n",
       "        [   3]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_tensor(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The model we used is copied straight from the one presented in the reference tutorial (https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n",
    "\n",
    "Note that attention is not implemented yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocabulary_size, hidden_size, embedding_size): # isn't vocab size = 1?\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_size) # TODO: load pretrained\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        embedded = self.embedding(input_data).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocabulary_size, hidden_size, embedding_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocabulary_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        output = self.embedding(input_data).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Here we define a function to process training for a single pair of sentences.\n",
    "\n",
    "**TODO** implement batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_FORCING_RATIO = 0.5\n",
    "MAX_LENGTH = 500\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=DEVICE)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')]], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "#     use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "    use_teacher_forcing = True\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization\n",
    "\n",
    "This instantiate a fresh model. You should run this cell **once** before running your training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "VOCABULARY_SIZE = vocabulary.size() + 1\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "encoder = EncoderRNN(VOCABULARY_SIZE, HIDDEN_SIZE, EMBEDDING_SIZE).to(DEVICE)\n",
    "decoder = DecoderRNN(VOCABULARY_SIZE, HIDDEN_SIZE, EMBEDDING_SIZE).to(DEVICE)\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch\n",
    "Here we run a training Epoch, that is, we run the whole dataset through the training procedure. This cell can be executed many times to run multiple Epochs (be careful not to re-initialize the model across Epochs: that would reset training to Epoch 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:04, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 5.194382742745282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:09, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 5.129294074950496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "149it [00:13, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 5.166470740610549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:17, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 5.2998507985255126\n",
      "0:00:17.734081\n",
      "202 0.09700315475463867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_EVERY = 50\n",
    "\n",
    "i = 1\n",
    "tick = datetime.now()\n",
    "loss_sum = 0.0\n",
    "for input_sentence, target_sentence in tqdm(zip(X, y)):\n",
    "    input_tensor = sentence_to_tensor(input_sentence)\n",
    "    target_tensor = sentence_to_tensor(target_sentence)\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    loss_sum += loss\n",
    "    if i%PRINT_EVERY == 0:\n",
    "        print(i, loss_sum/PRINT_EVERY)\n",
    "        loss_sum = 0.0\n",
    "    i += 1\n",
    "tock = datetime.now()\n",
    "\n",
    "epoch += 1\n",
    "\n",
    "print(tock-tick)\n",
    "print(i, loss_sum/PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(input_sentence, encoder, decoder):\n",
    "    result = []\n",
    "    \n",
    "    input_tensor = sentence_to_tensor(input_sentence)\n",
    "    input_length = input_tensor.size(0)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    for ei in range(input_length):\n",
    "        _, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')]], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(MAX_LENGTH):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        predicted_index = decoder_input.item()\n",
    "        \n",
    "        if predicted_index == vocabulary.index('<EOS>'):\n",
    "            break\n",
    "        result.append(vocabulary.word(predicted_index))\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it 's not bad !\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer(\"what's the meaning of life?'\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
