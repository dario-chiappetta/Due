{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder/Decoder Dialogue Management\n",
    "\n",
    "Here we use a simple Encoder/Decoder GRU network to predict answers from the Cornell Movie-Dialog Corpus. We use **PyTorch** as a deep learning framework.\n",
    "\n",
    "Most of the code in this notebook comes from the following **tutorial** on English-French translation.\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "We apply the Machine Translation framework described in the tutorial to Dialogue Management by processing sentences in the corpus by pairs: we encode the sentence, and decode the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from six import iteritems\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Load Cornell dataset \n",
    "\n",
    "We start loading the corpus' dialogs as **Episodes** (class due.episode.Episode). We limit the number of episodes to load so we can test the code more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2f61146c4645f2b15245cf33fcada9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=83097), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from due.corpora import cornell\n",
    "import itertools\n",
    "\n",
    "N_DIALOGS = 100\n",
    "\n",
    "episodes = list(itertools.islice(cornell.episode_generator(), N_DIALOGS))\n",
    "\n",
    "# episodes = cornell.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Event(type=<Type.Utterance: 'utterance'>, timestamp=datetime.datetime(2011, 6, 15, 12, 4, 49), agent='u9', payload=\"What's the worst?\"),\n",
       " Event(type=<Type.Utterance: 'utterance'>, timestamp=datetime.datetime(2011, 6, 15, 12, 4, 50), agent='u2', payload='You get the girl.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[95].events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (alternative) Load Star Wars Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from due.episode import Episode\n",
    "\n",
    "saved_episodes_filename = 'SW_EPISODES.pkl'\n",
    "\n",
    "with open(saved_episodes_filename, 'rb') as f:\n",
    "    saved_episodes = pickle.load(f)\n",
    "    \n",
    "episodes = [Episode.load(e) for e in saved_episodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning\n",
    "\n",
    "Here we define functions for a simple text processing pipeline, where we just convert sentences to lowercase and tokenize them using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from due.nlp.preprocessing import normalize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad . again .\n"
     ]
    }
   ],
   "source": [
    "s_normalized = normalize_sentence(s, False)\n",
    "print(s_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "Here we generate a dataset of utterances and their responses. The **output** of this section is:\n",
    "\n",
    "* A list of utterances (`str`) `X`    \n",
    "* A list of responses (`str`) `y`, one per utterance in `X`.\n",
    "\n",
    "Example:\n",
    "\n",
    "* X: `[\"hi\", \"hello how are you?\", \"i'm fine thanks\", ...]`\n",
    "* y: `[\"hello how are you?\", \"i'm fine thanks\", \"good to hear\", ...]`\n",
    "\n",
    "Note that within an Episode `i`, `y_i` is just `X_i[1:]`. This is not true when `X` and `y` are obtained concatenating data from multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from due.event import Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       "  \"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       "  'Not the hacking and gagging and spitting part.  Please.'],\n",
       " [\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       "  'Not the hacking and gagging and spitting part.  Please.',\n",
       "  \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from due.episode import extract_utterance_pairs\n",
    "\n",
    "def _is_utterance(event):\n",
    "    return event.type == Event.Type.Utterance\n",
    "\n",
    "def extract_utterance_pairs(episode, preprocess_f=None):\n",
    "    \"\"\"\n",
    "    Process Events in an Episode, extracting all the Utterance Event pairs that\n",
    "    can be interpreted as one dialogue turn (ie. an Agent's utterance, and a\n",
    "    different Agent's response).\n",
    "\n",
    "    In particular, Event pairs are extracted from the Episode so that:\n",
    "\n",
    "    * Both Events are Utterances (currently, non-utterances will raise an exception)\n",
    "    * The second Event immediately follows the first\n",
    "    * The two Events are acted by two different Agents\n",
    "\n",
    "    This means that if an utterance has more than one answers, only the first\n",
    "    one will be included in the result.\n",
    "\n",
    "    If a `preprocess_f` function is specified, resulting utterances will be run\n",
    "    through this function before being returned. A LRU Cache is applied to\n",
    "    `preprocess_f`, as most sentences will be returned as both utterances and\n",
    "    answers/\n",
    "\n",
    "    Return two lists of the same length, so that each utterance `X_i` in the\n",
    "    first list has its response `y_i` in the second.\n",
    "\n",
    "    :param episode: an Episode\n",
    "    :type episode: :class:`due.episode.Episode`\n",
    "    :param preprocess_f: when given, sentences will be run through this function before being returned\n",
    "    :type preprocess_f: `func`\n",
    "    :return: a list of utterances and the list of their answers (one per utterance)\n",
    "    :rtype: (`list`, `list`)\n",
    "    \"\"\"\n",
    "    preprocess_f = lru_cache(4)(preprocess_f) if preprocess_f else lambda x: x\n",
    "    result_X = []\n",
    "    result_y = []\n",
    "    for e1, e2 in zip(episode.events, episode.events[1:]):\n",
    "        if not _is_utterance(e1) or not _is_utterance(e2):\n",
    "            raise NotImplementedError(\"Non-utterance Events are not supported yet\")\n",
    "\n",
    "        if e1.agent != e2.agent and e1.payload and e2.payload:\n",
    "            result_X.append(preprocess_f(e1.payload))\n",
    "            result_y.append(preprocess_f(e2.payload))\n",
    "\n",
    "    return result_X, result_y\n",
    "\n",
    "extract_utterance_pairs(episodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec45e51ce8343c79fe7d6142627c756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for e in tqdm(episodes):\n",
    "    try:\n",
    "        episode_X, episode_y = extract_utterance_pairs(e)\n",
    "    except AttributeError:\n",
    "        print(\"Skipping episode with events: %s\" % e.events)\n",
    "    X.extend(episode_X)\n",
    "    y.extend(episode_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "Here we index all the words in the corpus so that we can associate each word with a numeric ID, and vice versa.\n",
    "\n",
    "**TODO**: consider using torchtext instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from due.vocabulary import Vocabulary\n",
    "\n",
    "from due import __version__\n",
    "\n",
    "UNK = '<UNK>'\n",
    "SOS = '<SOS>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "class Vocabulary():\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.index_to_count = defaultdict(int)\n",
    "        self.current_index = 0\n",
    "\n",
    "        self.add_word(UNK) # Unknown token\n",
    "        self.add_word(SOS) # Start of String\n",
    "        self.add_word(EOS) # End of String\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Add a new word to the dictionary.\n",
    "\n",
    "        :param word: the word to add\n",
    "        :type word: `str`\n",
    "        \"\"\"\n",
    "        if word in self.word_to_index:\n",
    "            index = self.word_to_index[word]\n",
    "        else:\n",
    "            index = self.current_index\n",
    "            self.current_index += 1\n",
    "            self.word_to_index[word] = index\n",
    "            self.index_to_word[index] = word\n",
    "\n",
    "        self.index_to_count[index] += 1\n",
    "\n",
    "    def index(self, word):\n",
    "        \"\"\"\n",
    "        Retrieve a word's index in the Vocabulary. Return the index of the <UNK>\n",
    "        token if not present.\n",
    "\n",
    "        :param word: the word to look up\n",
    "        :type word: `str`\n",
    "        :return: the word's index if existing, *<UNK>*'s index otherwise\n",
    "        :rtype: `int`\n",
    "        \"\"\"\n",
    "        if word in self.word_to_index:\n",
    "            return self.word_to_index[word]\n",
    "        return self.word_to_index[UNK]\n",
    "\n",
    "    def word(self, index):\n",
    "        \"\"\"\n",
    "        Return the word corresponding to the given index\n",
    "\n",
    "        :param index: the index to look up\n",
    "        :type index: `int`\n",
    "        :return: the words corresponding to the given index\n",
    "        :rtype: `str`\n",
    "        \"\"\"\n",
    "        return self.index_to_word[index]\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Return the number of words in the Vocabulary\n",
    "\n",
    "        :return: number of words in the Vocabulary\n",
    "        :rtype: `int`\n",
    "        \"\"\"\n",
    "        return len(self.word_to_index)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Return a serializable `dict` representing the Vocabulary.\n",
    "\n",
    "        :return: a serializable representation of self\n",
    "        :rtype: `dict`\n",
    "        \"\"\"\n",
    "        return {\n",
    "            '_version': __version__,\n",
    "            'word_to_index': self.word_to_index,\n",
    "            'index_to_word': self.index_to_word,\n",
    "            'index_to_count': self.index_to_count,\n",
    "            'current_index': self.current_index,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def load(data):\n",
    "        result = Vocabulary()\n",
    "        result.word_to_index = data['word_to_index']\n",
    "        result.index_to_word = data['index_to_word']\n",
    "        result.index_to_count = data['index_to_count']\n",
    "        result.current_index = data['current_index']\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_full = Vocabulary()\n",
    "for sentence in set(X + y):\n",
    "    for word in sentence.split():\n",
    "        vocabulary_full.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1123"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_full.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_vocabulary(vocabulary, min_occurrences):\n",
    "    \"\"\"\n",
    "    Return a copy of the given vocabulary where words with less than\n",
    "    `min_occurrences` occurrences are removed.\n",
    "\n",
    "    :param vocabulary: a Vocabulary\n",
    "    :type vocabulary: :class:`due.nlp.vocabulary.Vocabulary`\n",
    "    :param min_occurrences: minimum number of occurrences for a word to be kept\n",
    "    :type min_occurrences: `int`\n",
    "    :return: a pruned copy of the given vocabulary\n",
    "    :rtype: :class:`due.nlp.vocabulary.Vocabulary`\n",
    "    \"\"\"\n",
    "    result = Vocabulary()\n",
    "    for index, count in iteritems(vocabulary.index_to_count):\n",
    "        if count >= min_occurrences:\n",
    "            result.add_word(vocabulary.word(index))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = prune_vocabulary(vocabulary_full, min_occurrences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "We could initialize the model's embedding layer with random weights, but we expect better results using pre-trained word embeddings instead. We chose **GloVe** 6B, 300d word vectors for this purpose.\n",
    "\n",
    "To set these vectors as default embeddings for our network we need to prepare a matrix of `(vocabulary_size, embedding_dim)` elements where the *i*-th row is the embedding vector of the word of index *i* in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from due import resource_manager\n",
    "rm = resource_manager\n",
    "\n",
    "def get_embedding_matrix(vocabulary, embeddings_stream, embedding_dim, stub=False):\n",
    "    \"\"\"\n",
    "    Return a N x D matrix, where N is the number of words in the vocabulary,\n",
    "    and D is the given embeddings' dimensionality. The *i*-th word in the matrix\n",
    "    contains the embedding of the word with index *i* in the Vocabulary.\n",
    "\n",
    "    Sample usage:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        with rm.open_resource_file('embeddings.glove6B', 'glove.6B.300d.txt') as f:\n",
    "            embedding_matrix = get_embedding_matrix(vocabulary, f, 300)\n",
    "\n",
    "    :param vocabulary: a Vocabulary\n",
    "    :type vocabulary: :class:`due.nlp.vocabulary.Vocabulary`\n",
    "    :param embeddings_stream: stream to a resource containing word embeddings in the word2vec format\n",
    "    :type embeddings_stream: *file*\n",
    "    :param embedding_dim: dimensionality of the embeddings\n",
    "    :type embedding_dim: `int`\n",
    "    :param stub: if True, return a random N x D matrix without reading the embedding source\n",
    "    :type stub: bool\n",
    "    \"\"\"\n",
    "    if stub:\n",
    "        return np.random.rand(vocabulary.size(), embedding_dim)\n",
    "\n",
    "    unk_index = vocabulary.index(UNK)\n",
    "    result = np.zeros((vocabulary.size(), 300))\n",
    "    for line in tqdm(embeddings_stream):\n",
    "        line_split = line.split()\n",
    "        word = line_split[0]\n",
    "        index = vocabulary.index(word)\n",
    "        if index != unk_index:\n",
    "            vector = [float(x) for x in line_split[1:]]\n",
    "            result[index, :] = vector\n",
    "    sos_index = vocabulary.index(SOS)\n",
    "    result[sos_index, :] = np.ones(300)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd66254693824c5aab1ec4eaf7f3b3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "with rm.open_resource_file('embeddings.glove6B', 'glove.6B.300d.txt') as f:\n",
    "    embedding_matrix = torch.FloatTensor(get_embedding_matrix(vocabulary, f, EMBEDDING_DIM), device=DEVICE)\n",
    "\n",
    "# embedding_matrix = torch.FloatTensor(get_embedding_matrix(vocabulary, None, EMBEDDING_DIM, stub=True), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([318, 300])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-by-1 training\n",
    "\n",
    "Here we define a simple model that can be trained one sentence pair at the time. To reduce training time and improve generalization capabilities, usually deep learning systems are trained in **batches**. Batch training is implemented later on in this Notebook.\n",
    "\n",
    "## Encoding\n",
    "\n",
    "Here we define a function to encode a sentence into a Torch tensor of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_tensor(sentence):\n",
    "    sentence_indexes = [vocabulary.index(w) for w in sentence.split()]\n",
    "    sentence_indexes.append(vocabulary.index('<EOS>'))\n",
    "    return torch.tensor(sentence_indexes, dtype=torch.long, device=DEVICE).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 172],\n",
       "        [ 128],\n",
       "        [ 173],\n",
       "        [  36],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [  81],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [  19],\n",
       "        [ 174],\n",
       "        [  96],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [ 175],\n",
       "        [  52],\n",
       "        [  30],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   2]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_tensor(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model we used is copied straight from the one presented in the reference tutorial (https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n",
    "\n",
    "Note that attention is not implemented yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_matrix):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_size) # random init\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "    \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        embedded = self.embedding(input_data).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_matrix):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "        vocabulary_size = self.embedding.num_embeddings\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocabulary_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        output = self.embedding(input_data).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Here we define a function to process training for a single pair of sentences.\n",
    "\n",
    "**TODO**: implement training with no teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_FORCING_RATIO = 0.5\n",
    "MAX_LENGTH = 500 # Will raise an error if a longer sentence is encountered\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=DEVICE)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')]], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "#     use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "    use_teacher_forcing = True\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization\n",
    "\n",
    "This instantiate a fresh model. You should run this cell **once** before running your training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "VOCABULARY_SIZE = vocabulary.size()\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "encoder = EncoderRNN(HIDDEN_SIZE, embedding_matrix).to(DEVICE)\n",
    "decoder = DecoderRNN(HIDDEN_SIZE, embedding_matrix).to(DEVICE)\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "Here we run a training Epoch, that is, we run the whole dataset through the training procedure. This cell can be executed many times to run multiple Epochs (be careful not to re-initialize the model across Epochs: that would reset training to Epoch 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2005de6fbc4ed98e87fbee0b95e869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 4.38477422140188\n",
      "100 4.276535991392023\n",
      "150 3.9896919449196018\n",
      "200 3.9999233545248303\n",
      "\n",
      "0:00:04.364822\n",
      "202 0.09455751691545759\n"
     ]
    }
   ],
   "source": [
    "PRINT_EVERY = 50\n",
    "\n",
    "i = 1\n",
    "tick = datetime.now()\n",
    "loss_sum = 0.0\n",
    "for input_sentence, target_sentence in tqdm(zip(X, y)):\n",
    "    input_tensor = sentence_to_tensor(input_sentence)\n",
    "    target_tensor = sentence_to_tensor(target_sentence)\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    loss_sum += loss\n",
    "    if i%PRINT_EVERY == 0:\n",
    "        print(i, loss_sum/PRINT_EVERY)\n",
    "        loss_sum = 0.0\n",
    "    i += 1\n",
    "tock = datetime.now()\n",
    "\n",
    "epoch += 1\n",
    "\n",
    "print(tock-tick)\n",
    "print(i, loss_sum/PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(input_sentence, vocabulary, encoder, decoder):\n",
    "    result = []\n",
    "    \n",
    "    input_tensor = sentence_to_tensor(input_sentence)\n",
    "    input_length = input_tensor.size(0)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    for ei in range(input_length):\n",
    "        _, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')]], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(MAX_LENGTH):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        predicted_index = decoder_input.item()\n",
    "        \n",
    "        if predicted_index == vocabulary.index('<EOS>'):\n",
    "            break\n",
    "        result.append(vocabulary.word(predicted_index))\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK> <UNK> <UNK>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer(\"what's the meaning of life?'\", vocabulary, encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch training\n",
    "\n",
    "Instead of feeding sentence pairs one by one, we want the training procedure to predict a number of samples before computing the loss and completing the optimization step. This is called batch training.\n",
    "\n",
    "The code below is inspired to https://github.com/pengyuchen/PyTorch-Batch-Seq2seq/blob/master/seq2seq_translation_tutorial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "Here we compare our model's output in the single-sentence case vs. batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake embedding layer\n",
    "embedding = nn.Embedding(5, 10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single sentence tensor\n",
    "sentence_indexes = [1, 2, 3]\n",
    "sentence_tensor = torch.tensor(sentence_indexes, dtype=torch.long, device=DEVICE).view(-1, 1)\n",
    "input_data = sentence_tensor[0]\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 4]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "# Batch tensor\n",
    "input_batch = torch.tensor([1, 4], device=DEVICE).view(-1, 1)\n",
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2059,  0.7485, -0.2218, -0.1869,  1.3227, -1.1729,  2.0848,\n",
       "          1.2498,  0.2586, -1.1215]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2059,  0.7485, -0.2218, -0.1869,  1.3227, -1.1729,  2.0848,\n",
       "           1.2498,  0.2586, -1.1215]],\n",
       "\n",
       "        [[-2.2759, -0.3418,  1.8060, -0.4991,  0.1636, -0.0080,  2.0393,\n",
       "          -0.5704, -0.6461, -1.0731]]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2059,  0.7485, -0.2218, -0.1869,  1.3227, -1.1729,  2.0848,\n",
       "           1.2498,  0.2586, -1.1215],\n",
       "         [-2.2759, -0.3418,  1.8060, -0.4991,  0.1636, -0.0080,  2.0393,\n",
       "          -0.5704, -0.6461, -1.0731]]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(input_batch).view(1, BATCH_SIZE, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "We still compare the mode's output with the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNNBatch(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_matrix):\n",
    "        super(EncoderRNNBatch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "    \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        \n",
    "    def forward(self, input_data, batch_size, hidden):\n",
    "        embedded = self.embedding(input_data).view(1, batch_size, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(32, embedding_matrix).to(DEVICE)\n",
    "encoder_batch = EncoderRNNBatch(32, embedding_matrix).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2904,  0.7393,  0.2004,  0.5207,  0.0064,  0.4498, -0.4413,\n",
       "           -0.4177, -0.3568,  0.2950, -0.0515,  0.3705,  0.1036,  0.3482,\n",
       "           -0.2456,  0.5237, -0.0519,  0.0148, -0.4948, -0.8401,  0.1975,\n",
       "           -0.4884, -0.5748, -0.2525,  0.0070, -0.0130, -0.0603, -0.4300,\n",
       "           -0.4581,  0.0922, -0.0019,  0.8785]]], device='cuda:0'),\n",
       " tensor([[[ 0.2904,  0.7393,  0.2004,  0.5207,  0.0064,  0.4498, -0.4413,\n",
       "           -0.4177, -0.3568,  0.2950, -0.0515,  0.3705,  0.1036,  0.3482,\n",
       "           -0.2456,  0.5237, -0.0519,  0.0148, -0.4948, -0.8401,  0.1975,\n",
       "           -0.4884, -0.5748, -0.2525,  0.0070, -0.0130, -0.0603, -0.4300,\n",
       "           -0.4581,  0.0922, -0.0019,  0.8785]]], device='cuda:0'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-by-1 model\n",
    "encoder_hidden = encoder.init_hidden()\n",
    "encoder(input_data, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.2505,  0.6989,  0.9150, -0.4300,  0.9483,  0.0257,  0.2656,\n",
       "           -0.4640,  0.0664, -0.7791,  0.3410,  0.4176,  0.0135,  0.0289,\n",
       "           -0.6601, -0.1947,  0.0682,  0.0915, -0.2327,  0.0325,  0.0919,\n",
       "           -0.0842,  0.0118, -0.8286, -0.1511,  0.0230,  0.2868,  0.0651,\n",
       "           -0.3213,  0.4261,  0.2259,  0.6567],\n",
       "          [-0.4840,  0.0463, -0.0754,  0.0220, -0.2606, -0.3520,  0.0553,\n",
       "           -0.0882, -0.3300, -0.1037, -0.1906, -0.4104, -0.2669,  0.1247,\n",
       "            0.1579, -0.0815,  0.1124,  0.2149,  0.3253, -0.3377,  0.3453,\n",
       "           -0.4039, -0.1282,  0.2433, -0.6175,  0.0853, -0.2708,  0.1928,\n",
       "           -0.2178,  0.0617, -0.1191,  0.0704]]], device='cuda:0'),\n",
       " tensor([[[ 0.2505,  0.6989,  0.9150, -0.4300,  0.9483,  0.0257,  0.2656,\n",
       "           -0.4640,  0.0664, -0.7791,  0.3410,  0.4176,  0.0135,  0.0289,\n",
       "           -0.6601, -0.1947,  0.0682,  0.0915, -0.2327,  0.0325,  0.0919,\n",
       "           -0.0842,  0.0118, -0.8286, -0.1511,  0.0230,  0.2868,  0.0651,\n",
       "           -0.3213,  0.4261,  0.2259,  0.6567],\n",
       "          [-0.4840,  0.0463, -0.0754,  0.0220, -0.2606, -0.3520,  0.0553,\n",
       "           -0.0882, -0.3300, -0.1037, -0.1906, -0.4104, -0.2669,  0.1247,\n",
       "            0.1579, -0.0815,  0.1124,  0.2149,  0.3253, -0.3377,  0.3453,\n",
       "           -0.4039, -0.1282,  0.2433, -0.6175,  0.0853, -0.2708,  0.1928,\n",
       "           -0.2178,  0.0617, -0.1191,  0.0704]]], device='cuda:0'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch model\n",
    "encoder_hidden_batch = encoder_batch.init_hidden(BATCH_SIZE)\n",
    "encoder_batch(input_batch, BATCH_SIZE, encoder_hidden_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNNBatch(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_matrix):\n",
    "        super(DecoderRNNBatch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "        vocabulary_size = self.embedding.num_embeddings\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocabulary_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data, batch_size, hidden):\n",
    "        output = self.embedding(input_data).view(1, batch_size, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary_size=10, embedding_dim=5\n",
    "toy_embedding_matrix = torch.FloatTensor(np.random.rand(10, 5), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(32, toy_embedding_matrix).to(DEVICE)\n",
    "decoder_batch = DecoderRNNBatch(32, toy_embedding_matrix).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.3077, -2.2111, -2.2156, -2.2956, -2.4506, -2.2708, -2.3239,\n",
       "          -2.4023, -2.1885, -2.3949]], device='cuda:0'),\n",
       " tensor([[[ 0.1360,  0.0870, -0.0024,  0.0316,  0.1015, -0.0511,  0.0635,\n",
       "           -0.0976, -0.1084, -0.2082, -0.0360, -0.0117, -0.0133, -0.0607,\n",
       "            0.0992, -0.0543,  0.0322, -0.1395,  0.0183, -0.0259, -0.1004,\n",
       "           -0.0098,  0.0439, -0.1008, -0.1686, -0.0923, -0.0575, -0.0014,\n",
       "            0.1309, -0.0245,  0.0161,  0.0658]]], device='cuda:0'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-by-1 model\n",
    "decoder_input = torch.tensor([[vocabulary.index('<SOS>')]], device=DEVICE)\n",
    "decoder_hidden = encoder_hidden\n",
    "decoder(decoder_input, decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.2213, -2.3815, -2.3156, -2.4651, -2.2650, -2.3476, -2.3385,\n",
       "          -2.1747, -2.3454, -2.2068],\n",
       "         [-2.2213, -2.3815, -2.3156, -2.4651, -2.2650, -2.3476, -2.3385,\n",
       "          -2.1747, -2.3454, -2.2068]], device='cuda:0'),\n",
       " tensor([[[-0.0825, -0.1793, -0.0929,  0.2017, -0.1630,  0.0250,  0.1831,\n",
       "            0.0766,  0.0568, -0.1865,  0.0479, -0.1454, -0.0610, -0.0613,\n",
       "           -0.1179,  0.0283,  0.0750,  0.0194,  0.1918, -0.2052, -0.1502,\n",
       "            0.1523, -0.0692, -0.1486, -0.0085,  0.0094,  0.0786,  0.0634,\n",
       "            0.0587, -0.1259, -0.0008, -0.0085],\n",
       "          [-0.0825, -0.1793, -0.0929,  0.2017, -0.1630,  0.0250,  0.1831,\n",
       "            0.0766,  0.0568, -0.1865,  0.0479, -0.1454, -0.0610, -0.0613,\n",
       "           -0.1179,  0.0283,  0.0750,  0.0194,  0.1918, -0.2052, -0.1502,\n",
       "            0.1523, -0.0692, -0.1486, -0.0085,  0.0094,  0.0786,  0.0634,\n",
       "            0.0587, -0.1259, -0.0008, -0.0085]]], device='cuda:0'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch model\n",
    "decoder_input_batch = torch.tensor([[vocabulary.index('<SOS>')]*BATCH_SIZE], device=DEVICE)\n",
    "decoder_hidden_batch = encoder_hidden_batch\n",
    "decoder_batch(decoder_input_batch, BATCH_SIZE, decoder_hidden_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del encoder\n",
    "    del decoder\n",
    "    del decoder_batch\n",
    "    del encoder_hidden\n",
    "    del encoder_hidden_batch\n",
    "    del decoder_input\n",
    "    del decoder_hidden\n",
    "    del decoder_input_batch\n",
    "    del decoder_hidden_batch\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch iterator\n",
    "\n",
    "We want a function that takes our lists X and y and return them one batch at the time\n",
    "\n",
    "### batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    Generate two sequences of batches from the input lists `X` and `y`, so that\n",
    "    each batch contains `batch_size` elements.\n",
    "\n",
    "    >>> list(batches([0, 1, 2, 3, 4, 5, 6], ['a', 'b', 'c', 'd', 'e', 'f', 'g'], 3))\n",
    "    [([0, 1, 2], ['a', 'b', 'c']), ([3, 4, 5], ['d', 'e', 'f']), ([6], ['g'])]\n",
    "\n",
    "    :param X: a sequence of elements\n",
    "    :type X: `list`\n",
    "    :param y: a sequence of elements\n",
    "    :type y: `list`\n",
    "    :param batch_size: number of elements in each batch\n",
    "    :type batch_size: `int`\n",
    "    :return: a generator of the list of batches\n",
    "    :rtype: `list` of (`list`, `list`)\n",
    "    \"\"\"\n",
    "    for i in range(int(np.ceil(len(X)/batch_size))):\n",
    "        start_index = i*batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        yield X[start_index:end_index], y[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 1, 2], ['a', 'b', 'c']), ([3, 4, 5], ['d', 'e', 'f']), ([6], ['g'])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(batches([0, 1, 2, 3, 4, 5, 6], ['a', 'b', 'c', 'd', 'e', 'f', 'g'], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch to tensor\n",
    "Once we have a batch (a list of sentences), we want to turn it into something that can be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 172], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_tensor(X[0])[0] # Input of normal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 4]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch # What we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, pad_value, final_length):\n",
    "    \"\"\"\n",
    "    Trim the sequence if longer than final_length, pad it with pad_value if shorter.\n",
    "\n",
    "    In any case at lest one pad element will be left at the end of the sequence (this is\n",
    "    because we usually pad with the <EOS> token)\n",
    "\n",
    "    >>> pad_sequence([1, 2, 3], 0, 5)\n",
    "    [1, 2, 3, 0, 0]\n",
    "    >>> pad_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 0, 5)\n",
    "    [1, 2, 3, 4, 0]\n",
    "\n",
    "    :param sequence: any sequence of elements\n",
    "    :type sequence: `list`-like\n",
    "    :param pad_value: a value to pad the sequence with\n",
    "    :type pad_value: *any*\n",
    "    :param final_length: length of the final sequence\n",
    "    :type final_length: `int`\n",
    "    :return: the padded (or shortened) sequence, with at least one trailing `pad_value`\n",
    "    :rtype: `list`\n",
    "    \"\"\"\n",
    "    if len(sequence) >= final_length:\n",
    "        result = sequence[:final_length]\n",
    "        result[-1] = pad_value\n",
    "        return result\n",
    "\n",
    "    return sequence + [pad_value] * (final_length - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 0, 0]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence([1, 2, 3], 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [4]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "a = a.transpose()\n",
    "np.expand_dims(a, 2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch_to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensor(batch, vocabulary, max_words=None, device=None):\n",
    "    \"\"\"\n",
    "    Receive a list of sentences (strings), return a (*n_words* x *batch_size* x 1)\n",
    "    tensor `m`, so that `m[i]` contains an array `a` of *batch_size* rows and 1\n",
    "    column, so that `a[j]` contains the index of the `i`-th word in the `j`-th\n",
    "    sentence in the batch.\n",
    "\n",
    "    The **maximum number of words** in the sentences can be limited to\n",
    "    `max_word`. If `max_words` is not set, the limit will be set by the longest\n",
    "    sentence in the batch.\n",
    "\n",
    "    Sentences that are shorter than the maximum length in the resulting matrix\n",
    "    will be **padded** with EOS. At least one EOS token is appended to every\n",
    "    sentence in the resulting matrix.\n",
    "\n",
    "    :param batch: a list of sentence\n",
    "    :type batch: `list` of `str`\n",
    "    :param vocabulary: a Vocabulary to look up word indexes\n",
    "    :type vocabulary: :class:`due.nlp.vocabulary.Vocabulary`\n",
    "    :param max_words: sentences shorter than `max_words` will be trimmed\n",
    "    :type max_words: `int`\n",
    "    :param device: a Torch device to map the tensor to (eg. `torch.device(\"cuda\")`)\n",
    "    :type device: :class:`torch.device`\n",
    "    :return: a Torch tensor that is equivalent to the output of :func:`batch_to_matrix`\n",
    "    :rtype: :class:`torch.tensor`\n",
    "    \"\"\"\n",
    "    sentence_indexes = [[vocabulary.index(w) for w in sentence.split()] for sentence in batch]\n",
    "    max_length = max([len(x) for x in sentence_indexes])\n",
    "    if max_words:\n",
    "        max_length = min(max_length, max_words)\n",
    "    sentence_indexes = [pad_sequence(s, vocabulary.index(EOS), max_length+1) for s in sentence_indexes]\n",
    "\n",
    "    result = np.transpose(sentence_indexes)\n",
    "    result = np.expand_dims(result, axis=2)\n",
    "    return torch.tensor(result, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "3\n",
      "tensor([[ 36],\n",
      "        [ 36],\n",
      "        [  0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = ['this is a sentence', 'this is another much longer sentence', 'short sentence']\n",
    "batch_tensor = batch_to_tensor(batch, vocabulary, device=DEVICE)\n",
    "n_words = batch_tensor.size(0)\n",
    "batch_size = batch_tensor.size(1)\n",
    "first_word = batch_tensor[0]\n",
    "\n",
    "print(n_words)\n",
    "print(batch_size)\n",
    "print(first_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_FORCING_RATIO = 1.\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "def train_batch(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    batch_size = input_tensor.size(1)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden(batch_size)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=DEVICE)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], batch_size, encoder_hidden)\n",
    "#         encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')]*batch_size], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "#     use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "    use_teacher_forcing = True\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n",
    "#             print(\"decoder_output:\", decoder_output, decoder_output.size())\n",
    "#             print(\"target_tensor[di]:\", target_tensor[di], target_tensor[di].size())\n",
    "            loss += criterion(decoder_output, target_tensor[di].view(batch_size))\n",
    "            decoder_input = target_tensor[di]\n",
    "    \n",
    "    else:\n",
    "        eos_tensor = torch.tensor([vocabulary.index('<EOS>')], device=DEVICE)\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            predicted_words = target_tensor[di].view(batch_size)\n",
    "            loss += criterion(decoder_output, predicted_words)\n",
    "            if (predicted_words == eos_tensor*batch_size).all():\n",
    "                break\n",
    "            \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "VOCABULARY_SIZE = vocabulary.size()\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "encoder = EncoderRNNBatch(HIDDEN_SIZE, embedding_matrix).to(DEVICE)\n",
    "decoder = DecoderRNNBatch(HIDDEN_SIZE, embedding_matrix).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer_batch(input_sentence, vocabulary, encoder, decoder):\n",
    "    result = []\n",
    "    \n",
    "    input_tensor = batch_to_tensor([input_sentence], vocabulary, device=DEVICE)\n",
    "    input_length = input_tensor.size(0)\n",
    "    batch_size = input_tensor.size(1)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden(batch_size)\n",
    "    for ei in range(input_length):\n",
    "        _, encoder_hidden = encoder(input_tensor[ei], batch_size, encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')] * batch_size], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(MAX_LENGTH):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "#         print(decoder_output)\n",
    "        \n",
    "        predicted_index = decoder_input.item()\n",
    "        \n",
    "        if predicted_index == vocabulary.index('<EOS>'):\n",
    "            break\n",
    "        result.append(vocabulary.word(predicted_index))\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e411318def4ae790bd781833286ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0:00:00.267231\n",
      "5 4.3621409824916295\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "<UNK>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "PRINT_EVERY = 500\n",
    "EPOCHS = 1\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    i = 1\n",
    "    tick = datetime.now()\n",
    "    loss_sum = 0.0\n",
    "    loss_sum_partial = 0.0\n",
    "    for input_batch, target_batch in tqdm(batches(X, y, BATCH_SIZE)):\n",
    "        input_tensor = batch_to_tensor(input_batch, vocabulary, MAX_LENGTH, device=DEVICE)\n",
    "        target_tensor = batch_to_tensor(target_batch, vocabulary, MAX_LENGTH, device=DEVICE)\n",
    "\n",
    "        loss = train_batch(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        loss_sum += loss\n",
    "        loss_sum_partial += loss\n",
    "        if i%PRINT_EVERY == 0:\n",
    "            print(i, loss_sum_partial/PRINT_EVERY)\n",
    "            loss_sum_partial = 0.0\n",
    "        i += 1\n",
    "    tock = datetime.now()\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    print(tock-tick)\n",
    "    print(i, loss_sum/i)\n",
    "    print(predict_answer_batch(\"hi\", vocabulary, encoder, decoder))\n",
    "    print(predict_answer_batch(\"how are you?\", vocabulary, encoder, decoder))\n",
    "    print(predict_answer_batch(\"what's your name?\", vocabulary, encoder, decoder))\n",
    "    print(predict_answer_batch(\"My name is Anna\", vocabulary, encoder, decoder))\n",
    "    print(predict_answer_batch(\"What's the meaning of life?\", vocabulary, encoder, decoder))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model serialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"encdec-cornell-TEST\"\n",
    "model_filename = \"%s_MODEL.pt\" % MODEL_NAME\n",
    "dataset_filename = \"%s_DATASET.pt\" % MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'encoder': encoder.state_dict(),\n",
    "    'decoder': decoder.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'embedding_matrix': embedding_matrix\n",
    "}\n",
    "\n",
    "torch.save(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"X\": X,\n",
    "    \"y\": y,\n",
    "    \"vocabulary\": vocabulary.save()\n",
    "}\n",
    "\n",
    "torch.save(dataset, dataset_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from due.nlp.preprocessing import normalize_sentence\n",
    "from due.nlp.vocabulary import Vocabulary, get_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_deserialized = torch.load(dataset_filename)\n",
    "\n",
    "X_deserialized = dataset_deserialized[\"X\"]\n",
    "y_deserialized = dataset_deserialized[\"y\"]\n",
    "vocabulary_deserialized = Vocabulary.load(dataset_deserialized['vocabulary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deserialized = torch.load(model_filename)\n",
    "\n",
    "embedding_matrix_deserialized = model_deserialized['embedding_matrix']\n",
    "\n",
    "encoder_deserialized = EncoderRNNBatch(HIDDEN_SIZE, embedding_matrix_deserialized).to(DEVICE)\n",
    "encoder_deserialized.load_state_dict(model_deserialized['encoder'])\n",
    "\n",
    "decoder_deserialized = DecoderRNNBatch(HIDDEN_SIZE, embedding_matrix_deserialized).to(DEVICE)\n",
    "decoder_deserialized.load_state_dict(model_deserialized['decoder'])\n",
    "\n",
    "epoch_deserialized = model_deserialized['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from due.nlp.batches import batches, pad_sequence, batch_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_deserialized\n",
    "y = y_deserialized\n",
    "vocabulary = vocabulary_deserialized\n",
    "\n",
    "encoder = encoder_deserialized\n",
    "decoder = decoder_deserialized\n",
    "\n",
    "epoch = epoch_deserialized\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "VOCABULARY_SIZE = vocabulary.size()\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "embedding_matrix = embedding_matrix_deserialized\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
