{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder/Decoder Dialogue Management\n",
    "\n",
    "Here we use a simple Encoder/Decoder GRU network to predict answers from the Cornell Movie-Dialog Corpus. We use **PyTorch** as a deep learning framework.\n",
    "\n",
    "Most of the code in this notebook comes from the following **tutorial** on English-French translation.\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "We apply the Machine Translation framework to Dialogue Management by processing sentences in the corpus by pairs: we encode the sentence, and decode the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset \n",
    "\n",
    "We start loading the corpus' dialogs as **Episodes** (class due.episode.Episode). We limit the number of episodes to load so we can test the code more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/83097 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from due.corpora import cornell\n",
    "import itertools\n",
    "\n",
    "N_DIALOGS = 100\n",
    "\n",
    "episodes = list(itertools.islice(cornell.episode_generator(), N_DIALOGS))\n",
    "\n",
    "# episodes = cornell.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Event(type=<Type.Utterance: 'utterance'>, timestamp=datetime.datetime(2011, 6, 15, 12, 4, 49), agent='u9', payload=\"What's the worst?\"),\n",
       " Event(type=<Type.Utterance: 'utterance'>, timestamp=datetime.datetime(2011, 6, 15, 12, 4, 50), agent='u2', payload='You get the girl.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[95].events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning\n",
    "\n",
    "Here we define functions for a simple text processing pipeline, where we just convert sentences to lowercase and tokenize them using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "# Needed: pipenv run python -m spacy download en\n",
    "spacy_nlp_en = spacy.load('en')\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    s_spacy = spacy_nlp_en(sentence)\n",
    "    return [str(token) for token in s_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad . again .\n"
     ]
    }
   ],
   "source": [
    "def normalize_sentence(sentence, return_tokens=False):\n",
    "    result = sentence.lower()\n",
    "    result = re.sub(r'\\s+', ' ', result)\n",
    "    result = tokenize_sentence(result)\n",
    "    if not return_tokens:\n",
    "        result = ' '.join(result)\n",
    "    return result\n",
    "\n",
    "s_normalized = normalize_sentence(s, False)\n",
    "print(s_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "Here we generate a dataset of utterances and their responses. The **output** of this section is:\n",
    "\n",
    "* A list of utterances (`str`) `X`    \n",
    "* A list of responses (`str`) `y`, one per utterance in `X`.\n",
    "\n",
    "Example:\n",
    "\n",
    "* X: `[\"hi\", \"hello how are you?\", \"i'm fine thanks\", ...]`\n",
    "* y: `[\"hello how are you?\", \"i'm fine thanks\", \"good to hear\", ...]`\n",
    "\n",
    "Note that within an Episode `i`, `y_i` is just `X_i[1:]`. This is not true when `X` and `y` are obtained concatenating data from multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from due.event import Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad . again .',\n",
       "  \"well , i thought we 'd start with pronunciation , if that 's okay with you .\",\n",
       "  'not the hacking and gagging and spitting part . please .'],\n",
       " [\"well , i thought we 'd start with pronunciation , if that 's okay with you .\",\n",
       "  'not the hacking and gagging and spitting part . please .',\n",
       "  \"okay ... then how 'bout we try out some french cuisine . saturday ? night ?\"])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _is_utterance(event):\n",
    "    return event.type == Event.Type.Utterance\n",
    "\n",
    "def extract_pairs(episode):\n",
    "    \"\"\"\n",
    "    Process Events in an Episode, extracting all the Event pairs that can be interpreted as one\n",
    "    dialogue turn (ie. an Agent's utterance, and another Agent's response)\n",
    "    \n",
    "    In particular, Event pairs are extracted so that:\n",
    "    \n",
    "    * Both Events are Utterances (currently, non-utterances will raise an exception)\n",
    "    * The second Event immediately follows the first\n",
    "    * The two Events are acted by two different Agents\n",
    "    \n",
    "    Two lists of the same length are returned, so that each utterance (`str`) in the first list\n",
    "    has its response in the second.\n",
    "    \"\"\"\n",
    "    alternate_sentences = [episode.events[0].payload]\n",
    "    for e1, e2 in zip(episode.events, episode.events[1:]):\n",
    "        if not _is_utterance(e1) or not _is_utterance(e2):\n",
    "            raise NotImplementedError(\"Non-utterance Events are not supported yet\")\n",
    "            \n",
    "        if e1.agent != e2.agent:\n",
    "            alternate_sentences.append(e2.payload)\n",
    "\n",
    "    normalized_alternate_sentences = [normalize_sentence(s) for s in alternate_sentences]\n",
    "\n",
    "    result_X = normalized_alternate_sentences[0:-1]\n",
    "    result_y = normalized_alternate_sentences[1:]\n",
    "    \n",
    "    return result_X, result_y\n",
    "    \n",
    "extract_pairs(episodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7f2f19240748fc86f9e05ef7cfb268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for e in tqdm(episodes):\n",
    "    try:\n",
    "        episode_X, episode_y = extract_pairs(e)\n",
    "    except AttributeError:\n",
    "        print(\"Skipping episode with events: %s\" % e.events)\n",
    "    X.extend(episode_X)\n",
    "    y.extend(episode_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "Here we index all the words in the corpus so that we can associate each word with a numeric ID, and vice versa.\n",
    "\n",
    "**TODO**: consider using torchtext instead\n",
    "\n",
    "**TODO**: implement pruning of rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Vocabulary():\n",
    "    def __init__(self):\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.index_to_count = defaultdict(int)\n",
    "        self.current_index = 0\n",
    "        \n",
    "        self.add_word('<UNK>') # Unknown token\n",
    "        self.add_word('<SOS>') # Start of String\n",
    "        self.add_word('<EOS>') # End of String\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Add a new word to the dictionary.\n",
    "        \"\"\"\n",
    "        if word in self.word_to_index:\n",
    "            index = self.word_to_index[word]\n",
    "        else:\n",
    "            index = self.current_index\n",
    "            self.current_index += 1\n",
    "            self.word_to_index[word] = index\n",
    "            self.index_to_word[index] = word\n",
    "            \n",
    "        self.index_to_count[index] += 1\n",
    "        \n",
    "    def index(self, word):\n",
    "        \"\"\"\n",
    "        Retrieve a word's index in the Vocabulary. Return the index of the <UNK>\n",
    "        token if not present.\n",
    "        \"\"\"\n",
    "        if word in self.word_to_index:\n",
    "            return self.word_to_index[word]\n",
    "        return self.word_to_index['<UNK>']\n",
    "    \n",
    "    def word(self, index):\n",
    "        \"\"\"\n",
    "        Return the word corresponding to the given index/\n",
    "        \"\"\"\n",
    "        return self.index_to_word[index]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_full = Vocabulary()\n",
    "for sentence in set(X + y):\n",
    "    for word in sentence.split():\n",
    "        vocabulary_full.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_full.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import iteritems\n",
    "\n",
    "MIN_OCCURRENCES = 2\n",
    "\n",
    "vocabulary = Vocabulary()\n",
    "for index, count in iteritems(vocabulary_full.index_to_count):\n",
    "    if count >= MIN_OCCURRENCES:\n",
    "        vocabulary.add_word(vocabulary_full.word(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "We could initialize the model's embedding layer with random weights, but we expect better results using pre-trained word embeddings instead. We chose **GloVe** 6B, 300d word vectors for this purpose.\n",
    "\n",
    "To set these vectors as default embeddings for our network we need to prepare a matrix of `(vocabulary_size, embedding_dim)` elements where the *i*-th row is the embedding vector of the word of index *i* in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from due import resource_manager\n",
    "rm = resource_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(vocabulary, stub=False):\n",
    "    if stub:\n",
    "        return torch.tensor(np.random.rand(vocabulary.size(), EMBEDDING_DIM), device=DEVICE)\n",
    "\n",
    "    with rm.open_resource_file('embeddings.glove6B', 'glove.6B.300d.txt') as f:\n",
    "        unk_index = vocabulary.index('<UNK>')\n",
    "        result = np.zeros((vocabulary.size(), 300))\n",
    "        for line in tqdm(f):\n",
    "            line_split = line.split()\n",
    "            word = line_split[0]\n",
    "            index = vocabulary.index(word)\n",
    "            if index != unk_index:\n",
    "                vector = [float(x) for x in line_split[1:]]\n",
    "                result[index,:] = vector\n",
    "        sos_index = vocabulary.index('<SOS>')\n",
    "        result[sos_index,:] = np.ones(300)\n",
    "        return torch.FloatTensor(result, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d2a49aaa104765aee6d0ecca45661e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = get_embedding_matrix(vocabulary)\n",
    "# embedding_matrix = get_embedding_matrix(vocabulary, stub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([287, 300])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-by-1 training\n",
    "\n",
    "Here we define a simple model that can be trained one sentence pair at the time. To drastically improve training time, usually deep learning systems are trained in **batches**. Batch training is implemented later on in this Notebook.\n",
    "\n",
    "## Encoding\n",
    "\n",
    "Here we define a function to encode a sentence into a Torch tensor of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_tensor(sentence):\n",
    "    sentence_indexes = [vocabulary.index(w) for w in sentence.split()]\n",
    "    sentence_indexes.append(vocabulary.index('<EOS>'))\n",
    "    return torch.tensor(sentence_indexes, dtype=torch.long, device=DEVICE).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 195],\n",
       "        [  17],\n",
       "        [  50],\n",
       "        [ 193],\n",
       "        [   0],\n",
       "        [  12],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [  67],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [  96],\n",
       "        [ 233],\n",
       "        [ 180],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   0],\n",
       "        [   9],\n",
       "        [  93],\n",
       "        [  58],\n",
       "        [   0],\n",
       "        [   5],\n",
       "        [ 252],\n",
       "        [   5],\n",
       "        [   2]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_tensor(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model we used is copied straight from the one presented in the reference tutorial (https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html).\n",
    "\n",
    "Note that attention is not implemented yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_matrix):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_size) # random init\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "    \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        embedded = self.embedding(input_data).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_matrix):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "        vocabulary_size = self.embedding.num_embeddings\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocabulary_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data, hidden):\n",
    "        output = self.embedding(input_data).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Here we define a function to process training for a single pair of sentences.\n",
    "\n",
    "**TODO**: implement training with no teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_FORCING_RATIO = 0.5\n",
    "MAX_LENGTH = 500 # Will raise an error if a longer sentence is encountered\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=DEVICE)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')]], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "#     use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "    use_teacher_forcing = True\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization\n",
    "\n",
    "This instantiate a fresh model. You should run this cell **once** before running your training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "VOCABULARY_SIZE = vocabulary.size()\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "encoder = EncoderRNN(VOCABULARY_SIZE, embedding_matrix).to(DEVICE)\n",
    "decoder = DecoderRNN(VOCABULARY_SIZE, embedding_matrix).to(DEVICE)\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "Here we run a training Epoch, that is, we run the whole dataset through the training procedure. This cell can be executed many times to run multiple Epochs (be careful not to re-initialize the model across Epochs: that would reset training to Epoch 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcdfd4e4a8c42c9bc8a60a09408e6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 4.740919136900429\n",
      "100 4.4504865207016175\n",
      "150 4.37611336638037\n",
      "200 4.3735499388217685\n",
      "\n",
      "0:00:03.594551\n",
      "202 0.09222966194152832\n"
     ]
    }
   ],
   "source": [
    "PRINT_EVERY = 50\n",
    "\n",
    "i = 1\n",
    "tick = datetime.now()\n",
    "loss_sum = 0.0\n",
    "for input_sentence, target_sentence in tqdm(zip(X, y)):\n",
    "    input_tensor = sentence_to_tensor(input_sentence)\n",
    "    target_tensor = sentence_to_tensor(target_sentence)\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    loss_sum += loss\n",
    "    if i%PRINT_EVERY == 0:\n",
    "        print(i, loss_sum/PRINT_EVERY)\n",
    "        loss_sum = 0.0\n",
    "    i += 1\n",
    "tock = datetime.now()\n",
    "\n",
    "epoch += 1\n",
    "\n",
    "print(tock-tick)\n",
    "print(i, loss_sum/PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model serialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"encdec-cornell-100-batch-glove6B_300-vmin2-e1\"\n",
    "model_filename = \"%s_MODEL.pt\" % MODEL_NAME\n",
    "dataset_filename = \"%s_DATASET.pt\" % MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'encoder': encoder.state_dict(),\n",
    "    'decoder': decoder.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'encoder_optimizer': encoder_optimizer,\n",
    "    'decoder_optimizer': decoder_optimizer,\n",
    "    'embedding_matrix': embedding_matrix\n",
    "}\n",
    "\n",
    "torch.save(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"X\": X,\n",
    "    \"y\": y,\n",
    "    \"vocabulary\": {\n",
    "        \"word_to_index\": vocabulary.word_to_index,\n",
    "        \"index_to_word\": vocabulary.index_to_word,\n",
    "        \"index_to_count\": vocabulary.index_to_count,\n",
    "        \"current_index\": vocabulary.current_index\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(dataset, dataset_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_deserialized = torch.load(dataset_filename)\n",
    "\n",
    "X_deserialized = dataset_deserialized[\"X\"]\n",
    "y_deserialized = dataset_deserialized[\"y\"]\n",
    "vocabulary_deserialized = Vocabulary()\n",
    "vocabulary_deserialized.word_to_index = dataset_deserialized['vocabulary']['word_to_index']\n",
    "vocabulary_deserialized.index_to_word = dataset_deserialized['vocabulary']['index_to_word']\n",
    "vocabulary_deserialized.index_to_count = dataset_deserialized['vocabulary']['index_to_count']\n",
    "vocabulary_deserialized.current_index = dataset_deserialized['vocabulary']['current_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deserialized = torch.load(model_filename)\n",
    "\n",
    "embedding_matrix_deserialized = model_deserialized['embedding_matrix']\n",
    "\n",
    "encoder_deserialized = EncoderRNN(vocabulary_deserialized.size(), embedding_matrix_deserialized).to(DEVICE)\n",
    "encoder_deserialized.load_state_dict(model_deserialized['encoder'])\n",
    "\n",
    "decoder_deserialized = DecoderRNN(vocabulary_deserialized.size(), embedding_matrix_deserialized).to(DEVICE)\n",
    "decoder_deserialized.load_state_dict(model_deserialized['decoder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(input_sentence, vocabulary, encoder, decoder):\n",
    "    result = []\n",
    "    \n",
    "    input_tensor = sentence_to_tensor(input_sentence)\n",
    "    input_length = input_tensor.size(0)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    for ei in range(input_length):\n",
    "        _, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')]], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(MAX_LENGTH):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        predicted_index = decoder_input.item()\n",
    "        \n",
    "        if predicted_index == vocabulary.index('<EOS>'):\n",
    "            break\n",
    "        result.append(vocabulary.word(predicted_index))\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i <UNK> <UNK>'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer(\"what's the meaning of life?'\", vocabulary, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i <UNK> <UNK>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_answer(\"what's the meaning of life?'\", vocabulary_deserialized, encoder_deserialized, decoder_deserialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch training\n",
    "\n",
    "Instead of feeding sentence pairs one by one, we want the training procedure to predict a number of samples before computing the loss and completing the optimization step. This is called batch training.\n",
    "\n",
    "The code below is inspired to https://github.com/pengyuchen/PyTorch-Batch-Seq2seq/blob/master/seq2seq_translation_tutorial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "Here we compare our model's output in the single-sentence case vs. batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake embedding layer\n",
    "embedding = nn.Embedding(5, 10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single sentence tensor\n",
    "sentence_indexes = [1, 2, 3]\n",
    "sentence_tensor = torch.tensor(sentence_indexes, dtype=torch.long, device=DEVICE).view(-1, 1)\n",
    "input_data = sentence_tensor[0]\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 4]], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "# Batch tensor\n",
    "input_batch = torch.tensor([1, 4], device=DEVICE).view(-1, 1)\n",
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5899, -0.4195,  0.4906,  0.1626,  0.0029,  0.6255, -1.0804,\n",
       "         -0.0071,  1.5118, -0.9721]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5899, -0.4195,  0.4906,  0.1626,  0.0029,  0.6255, -1.0804,\n",
       "          -0.0071,  1.5118, -0.9721]],\n",
       "\n",
       "        [[ 0.8736,  0.3196, -0.1022,  0.2273, -0.6255, -0.9996,  0.0735,\n",
       "           0.4913,  0.6050,  0.4745]]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5899, -0.4195,  0.4906,  0.1626,  0.0029,  0.6255, -1.0804,\n",
       "          -0.0071,  1.5118, -0.9721],\n",
       "         [ 0.8736,  0.3196, -0.1022,  0.2273, -0.6255, -0.9996,  0.0735,\n",
       "           0.4913,  0.6050,  0.4745]]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(input_batch).view(1, BATCH_SIZE, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "We still compare the mode's output with the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNNBatch(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_matrix):\n",
    "        super(EncoderRNNBatch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "    \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        \n",
    "    def forward(self, input_data, batch_size, hidden):\n",
    "        embedded = self.embedding(input_data).view(1, batch_size, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(32, embedding_matrix).to(DEVICE)\n",
    "encoder_batch = EncoderRNNBatch(32, embedding_matrix).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0228,  0.2981,  0.9194, -0.1953, -0.2722, -0.4606, -0.0468,\n",
       "           -0.1577,  0.0971, -0.0423, -0.4126,  0.8335,  0.5273, -0.1992,\n",
       "            0.0203,  0.7854, -0.1932,  0.0382,  0.0955, -0.0258, -0.2477,\n",
       "            0.5920, -0.0355,  0.0702, -0.0024, -0.0518,  0.2959, -0.0327,\n",
       "           -0.8324,  0.7245,  0.0966, -0.8010]]], device='cuda:0'),\n",
       " tensor([[[ 0.0228,  0.2981,  0.9194, -0.1953, -0.2722, -0.4606, -0.0468,\n",
       "           -0.1577,  0.0971, -0.0423, -0.4126,  0.8335,  0.5273, -0.1992,\n",
       "            0.0203,  0.7854, -0.1932,  0.0382,  0.0955, -0.0258, -0.2477,\n",
       "            0.5920, -0.0355,  0.0702, -0.0024, -0.0518,  0.2959, -0.0327,\n",
       "           -0.8324,  0.7245,  0.0966, -0.8010]]], device='cuda:0'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-by-1 model\n",
    "encoder_hidden = encoder.init_hidden()\n",
    "encoder(input_data, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2550, -0.1818,  0.0316,  0.1641, -0.1318, -0.8157, -0.1721,\n",
       "           -0.0246,  0.0438,  0.2603, -0.6466,  0.7489, -0.0013, -0.7385,\n",
       "           -0.1891, -0.0044,  0.0814, -0.1645, -0.2884,  0.0751, -0.1295,\n",
       "           -0.4015, -0.2971,  0.8273,  0.4685,  0.5808, -0.0511, -0.0893,\n",
       "           -0.1929, -0.2152,  0.1525, -0.2222],\n",
       "          [-0.3364,  0.0427, -0.0655, -0.0571,  0.0610,  0.2467, -0.3439,\n",
       "           -0.3327, -0.2260,  0.3126,  0.0580, -0.2173,  0.3389,  0.1444,\n",
       "            0.3097,  0.1450,  0.2398,  0.3871, -0.2337,  0.2830, -0.3261,\n",
       "           -0.0741, -0.0842,  0.2484,  0.0201,  0.2271,  0.1544, -0.1194,\n",
       "           -0.2921,  0.1589,  0.2274, -0.0239]]], device='cuda:0'),\n",
       " tensor([[[-0.2550, -0.1818,  0.0316,  0.1641, -0.1318, -0.8157, -0.1721,\n",
       "           -0.0246,  0.0438,  0.2603, -0.6466,  0.7489, -0.0013, -0.7385,\n",
       "           -0.1891, -0.0044,  0.0814, -0.1645, -0.2884,  0.0751, -0.1295,\n",
       "           -0.4015, -0.2971,  0.8273,  0.4685,  0.5808, -0.0511, -0.0893,\n",
       "           -0.1929, -0.2152,  0.1525, -0.2222],\n",
       "          [-0.3364,  0.0427, -0.0655, -0.0571,  0.0610,  0.2467, -0.3439,\n",
       "           -0.3327, -0.2260,  0.3126,  0.0580, -0.2173,  0.3389,  0.1444,\n",
       "            0.3097,  0.1450,  0.2398,  0.3871, -0.2337,  0.2830, -0.3261,\n",
       "           -0.0741, -0.0842,  0.2484,  0.0201,  0.2271,  0.1544, -0.1194,\n",
       "           -0.2921,  0.1589,  0.2274, -0.0239]]], device='cuda:0'))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch model\n",
    "encoder_hidden_batch = encoder_batch.init_hidden(BATCH_SIZE)\n",
    "encoder_batch(input_batch, BATCH_SIZE, encoder_hidden_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNNBatch(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_matrix):\n",
    "        super(DecoderRNNBatch, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        embedding_dim = self.embedding.embedding_dim\n",
    "        vocabulary_size = self.embedding.num_embeddings\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocabulary_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data, batch_size, hidden):\n",
    "        output = self.embedding(input_data).view(1, batch_size, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary_size=10, embedding_dim=5\n",
    "toy_embedding_matrix = torch.FloatTensor(np.random.rand(10, 5), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(32, toy_embedding_matrix).to(DEVICE)\n",
    "decoder_batch = DecoderRNNBatch(32, toy_embedding_matrix).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.4036, -2.1507, -2.2598, -2.2110, -2.4558, -2.2650, -2.1703,\n",
       "          -2.4602, -2.2721, -2.4422]], device='cuda:0'),\n",
       " tensor([[[ 0.1051, -0.0295,  0.0116,  0.0640,  0.1038, -0.0042, -0.0312,\n",
       "           -0.0931, -0.0646, -0.1191,  0.0821, -0.1534, -0.0853, -0.0444,\n",
       "           -0.0749, -0.0700, -0.1183, -0.0063,  0.0183, -0.0702,  0.0869,\n",
       "           -0.0172, -0.0663,  0.0524, -0.1017,  0.0631, -0.0713, -0.0294,\n",
       "           -0.0407, -0.0362,  0.0062, -0.0139]]], device='cuda:0'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-by-1 model\n",
    "decoder_input = torch.tensor([[vocabulary.index('<SOS>')]], device=DEVICE)\n",
    "decoder_hidden = encoder_hidden\n",
    "decoder(decoder_input, decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.3927, -2.2153, -2.5005, -2.2281, -2.2738, -2.3554, -2.2440,\n",
       "          -2.3097, -2.1736, -2.3767],\n",
       "         [-2.3927, -2.2153, -2.5005, -2.2281, -2.2738, -2.3554, -2.2440,\n",
       "          -2.3097, -2.1736, -2.3767]], device='cuda:0'),\n",
       " tensor([[[-0.0384, -0.0389,  0.1550,  0.1176, -0.1043,  0.0947,  0.0626,\n",
       "           -0.0606, -0.0417, -0.0600,  0.0924,  0.1496, -0.0804,  0.0299,\n",
       "            0.0715, -0.0259, -0.0535, -0.1762,  0.1166, -0.1030,  0.0852,\n",
       "            0.0671,  0.0429,  0.0611,  0.1685,  0.1322,  0.1156,  0.0138,\n",
       "            0.0451,  0.0209,  0.0868, -0.0031],\n",
       "          [-0.0384, -0.0389,  0.1550,  0.1176, -0.1043,  0.0947,  0.0626,\n",
       "           -0.0606, -0.0417, -0.0600,  0.0924,  0.1496, -0.0804,  0.0299,\n",
       "            0.0715, -0.0259, -0.0535, -0.1762,  0.1166, -0.1030,  0.0852,\n",
       "            0.0671,  0.0429,  0.0611,  0.1685,  0.1322,  0.1156,  0.0138,\n",
       "            0.0451,  0.0209,  0.0868, -0.0031]]], device='cuda:0'))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch model\n",
    "decoder_input_batch = torch.tensor([[vocabulary.index('<SOS>')]*BATCH_SIZE], device=DEVICE)\n",
    "decoder_hidden_batch = encoder_hidden_batch\n",
    "decoder_batch(decoder_input_batch, BATCH_SIZE, decoder_hidden_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del encoder\n",
    "    del decoder\n",
    "    del decoder_batch\n",
    "    del encoder_hidden\n",
    "    del encoder_hidden_batch\n",
    "    del decoder_input\n",
    "    del decoder_hidden\n",
    "    del decoder_input_batch\n",
    "    del decoder_hidden_batch\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(X, y, batch_size):\n",
    "    for i in range(int(np.ceil(len(X)/batch_size))):\n",
    "        start_index = i*batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        yield X[start_index:end_index], y[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0, 1], ['a', 'b']), ([2, 3], ['c', 'd']), ([4], ['e'])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(batches([0, 1, 2, 3, 4], ['a', 'b', 'c', 'd', 'e'], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 195], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_tensor(X[0])[0] # Input of normal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1],\n",
       "        [ 4]], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, pad_value, final_length):\n",
    "    \"\"\"\n",
    "    Trim the sequence if longer than final_length, pad it with pad_value if shorter.\n",
    "    \n",
    "    In any case at lest one pad element will be left at the end of the sequence (this is\n",
    "    because we pad with the <EOS> token)\n",
    "    \"\"\"\n",
    "    if len(sequence) >= final_length:\n",
    "        result = sequence[:final_length]\n",
    "        result[-1] = pad_value\n",
    "        return result\n",
    "\n",
    "    return sequence + [pad_value] * (final_length - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 0, 0]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence([1, 2, 3], 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 0]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [4]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "a = a.transpose()\n",
    "np.expand_dims(a, 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_tensor(batch, max_words=None):\n",
    "    \"\"\"\n",
    "    Receive a list of sentences (strings), return a batch\n",
    "    \"\"\"\n",
    "    sentence_indexes = [[vocabulary.index(w) for w in sentence.split()] for sentence in batch]\n",
    "    max_length = max([len(x) for x in sentence_indexes])\n",
    "    if max_words:\n",
    "        max_length = min(max_length, max_words)\n",
    "    sentence_indexes = [pad_sequence(s, vocabulary.index('<EOS>'), max_length+1) for s in sentence_indexes]\n",
    "    \n",
    "    result = np.transpose(sentence_indexes)\n",
    "    result = np.expand_dims(result, axis=2)\n",
    "    return torch.tensor(result, dtype=torch.long, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "3\n",
      "tensor([[ 193],\n",
      "        [ 193],\n",
      "        [   0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = ['this is a sentence', 'this is another much longer sentence', 'short sentence']\n",
    "batch_tensor = batch_to_tensor(batch)\n",
    "n_words = batch_tensor.size(0)\n",
    "batch_size = batch_tensor.size(1)\n",
    "first_word = batch_tensor[0]\n",
    "\n",
    "print(n_words)\n",
    "print(batch_size)\n",
    "print(first_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_FORCING_RATIO = 0.5\n",
    "MAX_LENGTH = 30\n",
    "\n",
    "def train_batch(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    batch_size = input_tensor.size(1)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden(batch_size)\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "#     encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=DEVICE)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], batch_size, encoder_hidden)\n",
    "#         encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')]*batch_size], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "#     use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "    use_teacher_forcing = True\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n",
    "#             print(\"decoder_output:\", decoder_output, decoder_output.size())\n",
    "#             print(\"target_tensor[di]:\", target_tensor[di], target_tensor[di].size())\n",
    "            loss += criterion(decoder_output, target_tensor[di].view(batch_size))\n",
    "#             loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "VOCABULARY_SIZE = vocabulary.size()\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "encoder = EncoderRNNBatch(VOCABULARY_SIZE, embedding_matrix).to(DEVICE)\n",
    "decoder = DecoderRNNBatch(VOCABULARY_SIZE, embedding_matrix).to(DEVICE)\n",
    "\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d270c0499c54393a32614670b56094c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0:00:00.279829\n",
      "5 4.192670440673828\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c8e26b369841b2a4b32d52b08c735d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0:00:00.264212\n",
      "5 2.5834801909744103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_EVERY = 100\n",
    "EPOCHS = 2\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    i = 1\n",
    "    tick = datetime.now()\n",
    "    loss_sum = 0.0\n",
    "    loss_sum_partial = 0.0\n",
    "    for input_batch, target_batch in tqdm(batches(X, y, BATCH_SIZE)):\n",
    "        input_tensor = batch_to_tensor(input_batch, MAX_LENGTH)\n",
    "        target_tensor = batch_to_tensor(target_batch, MAX_LENGTH)\n",
    "\n",
    "        loss = train_batch(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        loss_sum += loss\n",
    "        loss_sum_partial += loss\n",
    "        if i%PRINT_EVERY == 0:\n",
    "            print(i, loss_sum_partial/PRINT_EVERY)\n",
    "            loss_sum_partial = 0.0\n",
    "        i += 1\n",
    "    tock = datetime.now()\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    print(tock-tick)\n",
    "    print(i, loss_sum/i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer_batch(input_sentence, vocabulary, encoder, decoder):\n",
    "    result = []\n",
    "    \n",
    "    input_tensor = batch_to_tensor([input_sentence])\n",
    "    input_length = input_tensor.size(0)\n",
    "    batch_size = input_tensor.size(1)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden(batch_size)\n",
    "    for ei in range(input_length):\n",
    "        _, encoder_hidden = encoder(input_tensor[ei], batch_size, encoder_hidden)\n",
    "\n",
    "    decoder_input = torch.tensor([[vocabulary.index('<SOS>')] * batch_size], device=DEVICE)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(MAX_LENGTH):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "#         print(decoder_output)\n",
    "        \n",
    "        predicted_index = decoder_input.item()\n",
    "        \n",
    "        if predicted_index == vocabulary.index('<EOS>'):\n",
    "            break\n",
    "        result.append(vocabulary.word(predicted_index))\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(predict_answer_batch(\"hi\", vocabulary, encoder, decoder))\n",
    "print(predict_answer_batch(\"hello\", vocabulary, encoder, decoder))\n",
    "print(predict_answer_batch(\"what's your name?\", vocabulary, encoder, decoder))\n",
    "print(predict_answer_batch(\"my name is Anna\", vocabulary, encoder, decoder))\n",
    "print(predict_answer_batch(\"good to see you\", vocabulary, encoder, decoder))\n",
    "print(predict_answer_batch(\"i like wine, and you?\", vocabulary, encoder, decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
